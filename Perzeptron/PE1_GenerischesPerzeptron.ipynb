{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PE1 - Ein generisches Perzeptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um mit dem Perzeptron experimentieren zu können, implementieren wir es als generische Klasse, in der die Anzahl der Eingabe und Ausgabe-Neuronen beliebig definiert werden kann. Hier noch einmal die wichtigsten Eigenschaften:\n",
    "\n",
    "__Perzeptron-Regel (mit Bias)__ Der (binäre) Wert des Output-Neurons $o_j$ ergibt sich als gewichtete Summe der $N$ Input-Neuronen:\n",
    "\n",
    "$$\n",
    "o_j(x) = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\sum_{k=1}^N w_{jk} x_k + b> 0 \\\\\n",
    "0 & \\sum_{k=1}^N w_{jk} x_k + b\\leq 0 \\\\\n",
    "\\end{array}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "Der __Lernalgorithmus__ bleibt erhalten:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "w_{jk} &\\rightarrow w_{jk} + \\alpha (t_j - o_j) x_{k}\\\\\n",
    "b &\\rightarrow b + \\alpha (t -o )\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "Dabei kann der Ausdruck $(t_j - o_j)$, d.h. die Differenz zwischen erwartetem (Target-)Wert $t$ und Outputwert $o$, jeweils nur die Werte $\\pm 1$ und 0 annehmen.\n",
    "\n",
    "Was bedeutet der Ausdruck $w_{jk} + \\alpha (t_j - o_j) x_{k}$? Die $w_{jk}$ bilden Gewichtsmatrix, die die Gewichte aller Verbindungen von den Eingabe-Nodes $x_k$  zu den Ausgabe-Nodes $o_j$ enthält. Bei $K$ Eingabe-Nodes und $J$ Ausgabe-Nodes ist dies eine $K \\times J$-Matrix. Der Fehler $(t_j - o_j)$ bildet einen Vektor mit $J$ Komponenten, und die Eingabe-Nodes einen Vektor mit $K$ Komponenten. Wir müssen also aus dem Fehlervektor und dem Eingabevektor eine Matrix derselben Gestalt wie die Gewichtsmatrix bilden, d.h. jedes Element des Fehlervektors mit jedem Element des Eingabevektors _kombinieren_. Wir haben bereits das __Skalarprodukt__ kennengelernt, mit dem aus zwei Vektoren ein Skalar gebildet wird. Hier wollen wir nun aus zwei Vektoren eine Matrix erzeugen, und die dazugehörende mathematische Operation wird das __äußere Produkt__ oder __Tensorprodukt__ genannt (das Skalarprodukt wird dagegen auch als __inneres Produkt__ bezeichnet). Sind $W$ die Gewichtsmatrix und $(t-o)$ und $x$ die Fehler- bzw. Eingabevektoren, so schreibt sich die Lernregel kurz:\n",
    "\n",
    "$$ W \\rightarrow W + \\alpha (t-o) \\otimes x$$\n",
    "\n",
    "Man könnte nun mit zwei for-Schleifen über $j$ und $k$ die Komponenten einzeln bestimmen. NumPy unterstützt diese Operation aber wieder direkt mit einer eigenen Funktion namens __outer__. Damit können wir nun unser Perzeptron implementieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\" Ein generisches Perzeptron \"\"\"\n",
    "    \n",
    "    def __init__(self,*nodes: list[int]) -> None:\n",
    "        \"\"\" Initialisert das Perzeptron. Gewichte und Bias werden auf zufällige Werte zwischen -1 und +1 gesetzt. \"\"\"\n",
    "        input, output = nodes\n",
    "        self.w = np.random.uniform(-1,1,(input,output)).T\n",
    "        self.b = np.random.uniform(-1,1,output)    \n",
    "        pass\n",
    "\n",
    "    def train(self,input: list[int],target: list[int],alpha:float = 0.1,epochs: int = 1) ->  None:\n",
    "        \"\"\" \n",
    "        Trainiert das Perzeptron mit den übergebenen Daten input und den Zielwerten target gemäß dem Lernalgorithmus. \n",
    "        alpha bestimmt die Lernrate und epochs die Anzahl der Durchläufe. \n",
    "        \"\"\"\n",
    "        for _ in range(epochs):\n",
    "            for x, t in zip(input,target):  \n",
    "                error = (t - self.forward(x))     \n",
    "                self.w += alpha * np.outer(error,x)\n",
    "                self.b += alpha * error\n",
    "        pass\n",
    "\n",
    "    def forward(self,x: list[int]) -> list[int]:\n",
    "        \"\"\" \n",
    "        Berechnet das Ergebnis bei einem angelegten Input x. \n",
    "        Durch die Aktivierungsfunktion sind nur die Werte 0 (inaktiv) und 1 (aktiv) möglich.\n",
    "        \"\"\"\n",
    "        return self.activation(np.dot(self.w,x) + self.b) \n",
    "\n",
    "    def activation(self,signal: list[int]) -> int:\n",
    "        \"\"\" Die Aktivierungsfunktion testet, obL das Signal positiv ist (Perzeptron-Regel). \"\"\"\n",
    "        return (signal > 0).astype(int)    \n",
    "   \n",
    "    def test(self,input: list[int],target: list[int]) -> list[bool]:\n",
    "        \"\"\" \n",
    "        Testet die Funktion des Perzeptrons, indem alle durch die Eingabewerte input erzeugten Ausgaben mit den \n",
    "        Zielwerten target verglichen werden\n",
    "        \"\"\"\n",
    "        y = self.forward(input)\n",
    "           \n",
    "    def __str__(self) -> str:\n",
    "        return \"Gewichte: {0}, Bias = {1}\".format(self.w, self.b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwendung des Perzeptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir probieren unser generisches Perzeptron wieder an den logischen Schaltungen aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AND ----\n",
      "[0 0] -> [0]\n",
      "[0 1] -> [0]\n",
      "[1 0] -> [0]\n",
      "[1 1] -> [1]\n",
      "--- OR ----\n",
      "[0 0] -> [0]\n",
      "[0 1] -> [1]\n",
      "[1 0] -> [1]\n",
      "[1 1] -> [1]\n",
      "--- NAND ----\n",
      "[0 0] -> [1]\n",
      "[0 1] -> [1]\n",
      "[1 0] -> [1]\n",
      "[1 1] -> [0]\n",
      "--- XOR ----\n",
      "[0 0] -> [1]\n",
      "[0 1] -> [0]\n",
      "[1 0] -> [0]\n",
      "[1 1] -> [0]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[x,y] for x in [0,1] for y in [0,1]])\n",
    "\n",
    "logic_functions = {\"AND\": lambda a,b : a and b, \n",
    "                   \"OR\": lambda a,b : a or b, \n",
    "                   \"NAND\": lambda a,b : not (a and b), \n",
    "                   \"XOR\": lambda a,b : (a and not b) or (not a and b)}\n",
    "\n",
    "for n,f in logic_functions.items():\n",
    "    t = [[f(a,b)] for (a,b) in x]  \n",
    "    p = Perceptron(2,1)\n",
    "    p.train(x,t,epochs = 100)\n",
    "    print(f'--- {n} ----')\n",
    "    for tx in x:    \n",
    "        print(f'{tx} -> {p.forward(tx)}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Ergebnis ist wieder dasselbe, incl. XOR-Problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "Weitere Details findet man im Wikipedia-Artikel zum Perzeptron: https://de.wikipedia.org/wiki/Perzeptron"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
