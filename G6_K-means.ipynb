{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grundlagen 6: k-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Wir haben bereits durch Betrachtung der Daten des Iris-Dataset einen einfachen _Klassifizierer_ erstellt, ohne KI einzusetzen. \n",
    "\n",
    "Das Iris-Dataset ist ein schönes Beispiel, wie man eine gegebene Datenmenge in Gruppen, sogenannte **Cluster** einteilen kann, und an ihm kann man sehr gut ein Verfahren demonstrieren, der unter dem Namen **k-Means** bekannt ist. \n",
    "\n",
    "### Die Idee hinter k-Means\n",
    "\n",
    "Die Idee hinter diesem Verfahren ist, dass die zu unterscheidenden Klassen im Raum der gegebenen Parameter _eng zusammenliegen_.  Dies setzt voraus, dass es in diesem Raum eine __Metrik__ gibt, mit der Abstände gemessen werden können. Die vertrauteste Metrik ist die __Euklidische Metrik__, mit der wir auch im täglichen Leben Abstände messen. Wir denken uns zusammengehörde Werte in einem kugelförmigen Bereich zusammengedrängt. Innerhalb dieser Kugel liegen alle Werte, die vom Mittelpunkt der Kugel einen Abstand haben, der unterhalb des Radius der Kugel liegen. Es ist nun unsere Aufgabe, diese Mittelpunkte zu finden, die den Mittelwert (\"Mean\") des Clusters bilden.\n",
    "\n",
    "(In unserem heuristischen Ansatz bildeten unsere Cluster dagegen vierdimensionale Quader.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means: Mathematische Formulierung\n",
    "Ziel von k-Means ist es, einen Datensatz $x_1,\\ldots,x_N$ so in $k$ Cluster zu teilen, dass die Summe der quadrierten Abweichungen von den Schwerpunkten  $\\mu_i$ minimal ist. Mathematisch entspricht dies der Minimierung der Funktion\n",
    "\n",
    "$$ L = L(\\mu_i,C_i) := \\sum_{i=1}^k  \\sum_{x_j \\in C_i} |x_j - \\mu_i|^2 $$\n",
    "\n",
    "mit den Schwerpunkten $\\mu_i$ der Cluster $C_i$. Man beachte, dass $x_j$  und $\\mu_i$ Vektoren sind und $|x_j - \\mu_i|$ der Euklidische Abstand.\n",
    "\n",
    "Diese *Verlustfunktion* ist wieder eine Fehlerquadratsumme, allerdings diesmal eine Doppelsumme, da über die Cluster $C_i$ summiert wird und darin jeweils über die quadratischen Abweichungen vom Mittelwert $\\mu_j$, also die **Varianzen**. Daher spricht man auch von **Clustering durch Varianzminimierung**. \n",
    "\n",
    "Zu einer vorgegebnen Anzahl $k$ von Clustern ist also eine *optimale* Zuordnung der Daten zu den einzelnen Clustern gesucht, so dass die Varianz um die Mittelwerte (*means*) minimal ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der Lloyd-Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Der am häufigsten verwendete k-Means-Algorithmus ist der Lloyd-Algorithmus, der oft als „_der_ k-means-Algorithmus“ bezeichnet wird. Der Algorithmus funktioniert wie folgt:\n",
    "\n",
    "1. **Initialisierung**: Wähle zufällig $k$ Mittelwerte $m_1,\\ldots,,m_k$ aus den Daten\n",
    "2. **Zuordnung**: Jedes Datenobjekt wird demjenigen Cluster zugeordnet, bei dem die Cluster-Varianz am wenigsten erhöht wird:\n",
    "\n",
    "$$C_i := \\{ x_j :  |x_j-m_i|^2 \\leq |x_j-m_l|^2 \\;\\; \\text{für alle}\\;\\; l = 1,\\ldots k  \\}$$\n",
    "\n",
    "3. **Aktualisierung**: Berechne die Mittelpunkte (= Schwerpunkte) der Cluster neu:$$m_i \\rightarrow \\frac{1}{|C_i|} \\sum_{x_j \\in C_i} x_j $$\n",
    "4. **Wiederholung** der Schritte 2 und 3, bis sich die Zuordnungen nicht mehr ändern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Der Lloyd-Algorithmus am einfachen Beispiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir nehmen als eindimensionales Beispiel die drei Zahlen 1,3,8 und wollen sie in zwei Cluster einteilen, nämlich $C_1 = \\{1,3\\}$ und $C_2 = \\{8\\}$. \n",
    "\n",
    "1. **Initialisierung**: Um das Verfahren zu demonstrieren, wählen wir im ersten Schritte die zwei Cluster-Center $m_1 = 1$ und $m_2 = 3$. Dies ist der ungünstigste Fall, ist aber wegen der ansonsten zufälligen Wahl möglich (und eines der Probleme des Algorithmus).\n",
    "\n",
    "2. **Zuordnung**: Wir erhalten im ersten Durchlauf die Zuordnung $C_1 = \\{1\\}, C_2 = \\{3,8\\}$. Das ist nicht das, was wir wollen; 1 und 3  liegen in getrennten Clustern, und 3 und 8 im selben Cluster.\n",
    "\n",
    "3. **Aktualisierung**: $m_1 \\rightarrow 1, m_2 \\rightarrow \\frac{3 + 8} {2} = 5,5$. Der Mittelpunkt von $C_2$ wird vom Wert 8 \"angezogen\".\n",
    "\n",
    "4. **Wiederholung**: Da sich der Wert vom $m_2$ verändert hat, wiederholen wir Schritte 2 und 3.\n",
    "\n",
    "\n",
    "2. **Zuordnung**: Die neuen Cluster lauten (ohne große Rechnung) $C_1 = \\{1,2\\}, C_2 = \\{8\\}$. Die Cluster sind jetzt so wie gedacht.\n",
    "\n",
    "3. **Aktualisierung**: $m_1 \\rightarrow \\frac{1 + 3} {2} = 2, m_2 \\rightarrow 8$. \n",
    "\n",
    "4. **Wiederholung**: Da sich sowohl die Cluster als auch die Mittelpunkte verändert haben, wiederholen wir Schritte 2 und 3.\n",
    "\n",
    "\n",
    "2. **Zuordnung**: Die Cluster ändern sich nicht.\n",
    "\n",
    "3. **Aktualisierung**: Die Mittelpunkte ändern sich nicht.\n",
    "\n",
    "4. **Wiederholung**: Da sowohl die Cluster als auch die Mittelpunkte unverändert sind, endet das Verfahren mit den erwarteten Clustern.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementierung des Lloyd-Algorithmus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir schreiben diese Funktionen einmal ausführlich hin, obwohl wir sie auch aus einer Bibliothek laden könnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(xs,k):\n",
    "    ''' Wähle zufällig k Daten '''\n",
    "    return np.array(np.random.default_rng().choice(xs,k))\n",
    "\n",
    "def zuordnung(xs,ms):\n",
    "    '''Bildung der Cluster aus den Punken, abhängig vom aktuellen Mittelpunkt'''\n",
    "    cs = [[] for m in ms]\n",
    "\n",
    "    for x in xs:   \n",
    "        dist = [euklid(x,m) for m in ms]\n",
    "        index = dist.index(min(dist))  \n",
    "        cs[index].append(x)  \n",
    "     \n",
    "    return np.array([np.array([x for x in c]) for c in cs],dtype='object')\n",
    "\n",
    "def aktualisierung(cs):\n",
    "    '''Berechnet den neuen Cluster-Schwerpunkts'''\n",
    "    ms = [[] for _ in cs]\n",
    "    for n,c in enumerate(cs):\n",
    "        ms[n] = schwerpunkt(c)\n",
    "    return np.array(ms)\n",
    "\n",
    "def kmeans(xs,ms):\n",
    "    '''Der k-Means-Algorithmus; Startwerte ms sind vorgegeben'''\n",
    "    ms_alt = None\n",
    "    #while changed(ms,ms_alt):\n",
    "    for _ in range(200):\n",
    "        ms_alt = ms\n",
    "        cs = zuordnung(xs,ms)\n",
    "        ms = aktualisierung(cs)\n",
    "    return cs,ms\n",
    "\n",
    "def schwerpunkt(xs):\n",
    "    '''Berechnet den Schwerpunkt einer Menge von Vektoren'''\n",
    "    sum = np.zeros(xs.shape[1])\n",
    "    for x in xs:\n",
    "        sum += x\n",
    "    return sum / len(xs)\n",
    "    \n",
    "def euklid(x,y):\n",
    "    '''Berechnet das Quadrat des Euklidischen Abstands zwischen zwei Punkten.'''\n",
    "    return np.dot(x-y,x-y) ** 2\n",
    "\n",
    "\n",
    "def changed(ms,ms_alt):\n",
    "    if ms_alt is None:\n",
    "        return True\n",
    "    for m in ms:\n",
    "        for ma in ms_alt:\n",
    "            if np.array_equal(m,ma):\n",
    "                continue\n",
    "            return True \n",
    "    return False\n",
    "\n",
    "def print_clusters(cs,ms,elements=False):\n",
    "    '''Formatierte Ausgabe der Cluster'''\n",
    "    for n,(c,m) in enumerate(zip(cs,ms)):\n",
    "        print(f'Cluster {n} ({len(c)} Punkte): Mittelpunkt {m}')\n",
    "        if elements is True:\n",
    "            print(f'Punkte: {c}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unser einfaches Beispiel mit drei Punkten\n",
    "Wir testen unser Beispiel an der Punktmenge $\\{1,3,8\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (2 Punkte): Mittelpunkt [2.]\n",
      "Punkte: [[1]\n",
      " [3]]\n",
      "Cluster 1 (1 Punkte): Mittelpunkt [8.]\n",
      "Punkte: [[8]]\n"
     ]
    }
   ],
   "source": [
    "xs = [np.array([1]),np.array([3]),np.array([8])]\n",
    " \n",
    "ms = [xs[0],xs[1]] \n",
    "cs,ms = kmeans(xs,ms)\n",
    "\n",
    "print_clusters(cs,ms,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zur Geschichte\n",
    "Der Begriff „k-means“ wurde zuerst von MacQueen 1967 verwendet, die Idee geht jedoch auf Hugo Steinhaus 1957 zurück. Der heutzutage meist als „k-means-Algorithmus“ bezeichnete Standard-Algorithmus wurde 1957 von Lloyd vorgeschlagen, aber erst 1982 in einer Informatik-Zeitschrift publiziert. [Quelle: Wikipedia]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "* [K-Means in der Wikipedia](https://de.wikipedia.org/wiki/K-Means-Algorithmus)\n",
    "* [ELKI - in Java geschriebenes Framewok](https://de.wikipedia.org/wiki/Environment_for_DeveLoping_KDD-Applications_Supported_by_Index-Structures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
